{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "54733ee0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b543008b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pronouncing\n",
    "\n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "import torch\n",
    "\n",
    "from transformers.testing_utils import require_torch, torch_device\n",
    "import random\n",
    "\n",
    "from transformers.generation.logits_process import MinLengthLogitsProcessor, TopKLogitsWarper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa721c",
   "metadata": {},
   "source": [
    "To minimize computational cost and latency, we'll develop with `gpt2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "591ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f3266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# !pip install unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191a7d8",
   "metadata": {},
   "source": [
    "Define some functions for getting syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "f23d62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmu_syllable_counter(word):\n",
    "    \"\"\"\n",
    "    Returns inf for OOV tokens.\n",
    "    Note: This prohibits things like numbers and punctuation. Very naive and dumb.\n",
    "    \"\"\"\n",
    "    pronunciation_list = pronouncing.phones_for_word(word)\n",
    "    if len(pronunciation_list) > 0:\n",
    "        syllable_count = pronouncing.syllable_count(pronunciation_list[0])\n",
    "    else:\n",
    "        return float(\"Inf\")\n",
    "    \n",
    "    return syllable_count\n",
    "\n",
    "def syllable_mapper(vocab):\n",
    "    syllable_map = {}\n",
    "    for token, idx in vocab.items():\n",
    "        n_syllables = cmu_syllable_counter(token)\n",
    "        try:\n",
    "            syllable_map[n_syllables].append(idx)\n",
    "        except KeyError:\n",
    "            syllable_map[n_syllables] = [idx]\n",
    "    return syllable_map\n",
    "\n",
    "def token_syllable_scores(tokenizer, pt=True, free_tokens=['\\n', '!', ',', ':', '?', ';', ' ']):\n",
    "    \"\"\"\n",
    "    Returns list or torch tensor of size==tokenizer.vocab_size where element i is the count of syllables\n",
    "    for token i.\n",
    "    \"\"\"\n",
    "    sorted_vocab = {k: v for k, v in sorted(tokenizer.vocab.items(), key=lambda item: item[1])}\n",
    "    syllable_scores = []\n",
    "    for token, idx in sorted_vocab.items():\n",
    "\n",
    "        # Have to decode the vocab item to deal with special characters, e.g. '\\n' is represented as 'ÄŠ'\n",
    "        \n",
    "   \n",
    "        decoded_token = tokenizer.decode(sorted_vocab[token])\n",
    "        if decoded_token != ' ':\n",
    "            decoded_token = decoded_token.strip()\n",
    "        if decoded_token not in free_tokens:\n",
    "            n_syllables = cmu_syllable_counter(decoded_token)\n",
    "        else:\n",
    "            n_syllables = 0\n",
    "\n",
    "        syllable_scores.append(n_syllables)\n",
    "    \n",
    "    # Scoring is wrong for '\\n' patching\n",
    "    syllable_scores[tokenizer('\\n')['input_ids'][0]] = 0\n",
    "        \n",
    "    if pt:\n",
    "        return torch.Tensor(syllable_scores)\n",
    "    return syllable_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "fe6a6349-6566-4254-939e-f917915b5f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 386,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('\\n')['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "bb1b8353-910e-4e4e-ba21-64de03c270ab",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 383,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('\\n')['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "85bee2a4-b404-4106-b72a-a1dce5d297a7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "03b29e0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'TestTokenizer' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[385], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m syllable_scores[\u001b[38;5;241m4\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     31\u001b[0m test_syllable_mapper()\n\u001b[0;32m---> 32\u001b[0m \u001b[43mtest_token_syllable_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[385], line 25\u001b[0m, in \u001b[0;36mtest_token_syllable_scores\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m TestTokenizer()\n\u001b[1;32m     23\u001b[0m vocab \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliving\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mon\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m1\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;241m2\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroad\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorpus\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m3\u001b[39m}\n\u001b[0;32m---> 25\u001b[0m syllable_scores \u001b[38;5;241m=\u001b[39m \u001b[43mtoken_syllable_scores\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m syllable_scores[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m syllable_scores[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[0;32mIn[382], line 47\u001b[0m, in \u001b[0;36mtoken_syllable_scores\u001b[0;34m(tokenizer, pt, free_tokens)\u001b[0m\n\u001b[1;32m     44\u001b[0m     syllable_scores\u001b[38;5;241m.\u001b[39mappend(n_syllables)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Scoring is wrong for '\\n' patching\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m syllable_scores[\u001b[43mtokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pt:\n\u001b[1;32m     50\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mTensor(syllable_scores)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'TestTokenizer' object is not callable"
     ]
    }
   ],
   "source": [
    "def test_syllable_mapper():\n",
    "    vocab = {'living': 0, 'on': 1, 'the': 2, \"road\": 3}\n",
    "    \n",
    "    def convert_ids_to_tokens(token_id, vocab):\n",
    "        return [token for token in vocab if vocab[token]==token_id][0]\n",
    "    \n",
    "    syllable_map = syllable_mapper(vocab)\n",
    "    assert len(syllable_map[1]) == 3\n",
    "    assert convert_ids_to_tokens(syllable_map[2][0], vocab) == 'living'\n",
    "    \n",
    "\n",
    "\n",
    "class TestTokenizer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {'living': 0, 'on': 1, 'the': 2, \"road\": 5, \"!\": 4, \"corpus\": 3}\n",
    "    \n",
    "    def decode(self, token_id): \n",
    "        return [k for k,v in self.vocab.items() if v==token_id][0]\n",
    "    \n",
    "def test_token_syllable_scores():\n",
    "    tokenizer = TestTokenizer()\n",
    "    vocab = {'living': 0, 'on': 1, 'the': 2, \"road\": 5, \"!\": 4, \"corpus\": 3}\n",
    "    \n",
    "    syllable_scores = token_syllable_scores(tokenizer, pt=False)\n",
    "    assert syllable_scores[0] == 2\n",
    "    assert syllable_scores[1] == 1\n",
    "    assert syllable_scores[3] == 2\n",
    "    assert syllable_scores[4] == 0\n",
    "    \n",
    "test_syllable_mapper()\n",
    "test_token_syllable_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "33f06609",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_scores = token_syllable_scores(tokenizer, pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "7d567dcb-1477-46f4-a35e-c4e190e0e967",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable_scores[198]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb00b0d",
   "metadata": {},
   "source": [
    "Let's demonstrate how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f16dfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A word with the max number of syllables is 'homosexuality', which has inf syllables. It has a score of 7.0.\n"
     ]
    }
   ],
   "source": [
    "# Bug now that max is inf\n",
    "inf_mask = syllable_scores == float(\"Inf\")\n",
    "temp_scores = syllable_scores.masked_fill(inf_mask, -float('Inf'))\n",
    "max_word = temp_scores.argmax()\n",
    "word = tokenizer.decode(max_word.item())\n",
    "print(f\"A word with the max number of syllables is '{word.strip()}', which has {cmu_syllable_counter(word)} syllables. It has a score of {temp_scores[max_word.item()]}.\")\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2604da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0d08693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Happy birthday to you\n",
    "Happy birthday to you\n",
    "Happy birthday dear John\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4ce3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_init = \"\"\"Happy birthday to you\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570b05b",
   "metadata": {},
   "source": [
    "Here's some code I wrote based on the `poesy` package that returns syllable counts by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fcaf1927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_init:  \"Happy birthday to you\" \n",
      "Syllable budget tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "# from poesy import Poem\n",
    "from bragi.verse_parsers import PoesyParsedVerseHandler\n",
    "verse_handler = PoesyParsedVerseHandler()\n",
    "_, syllable_budget = verse_handler.example(text_init)\n",
    "syllable_budget = torch.Tensor(syllable_budget)\n",
    "print('text_init: ', f'\"{text_init}\"', '\\nSyllable budget', syllable_budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac4bd1",
   "metadata": {},
   "source": [
    "Here, I define a LogitsWarper that tracks syllable counts by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a19f8d4c-2daa-49cf-8870-406a81b17687",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "198"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer('\\n', return_tensors='pt')['input_ids'].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "d00b96fe-be7a-45ab-a49a-5946d50496f0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(198)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 782,
   "id": "2e16e7c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers.generation.logits_process import LogitsWarper\n",
    "\n",
    "# TODO\n",
    "# 1. Add support for beam search\n",
    "class SyllableRestrictionWarper(LogitsWarper):\n",
    "    def __init__(\n",
    "        self, \n",
    "        prompt: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
    "        syllable_budget: torch.Tensor,\n",
    "        syllable_scorer: callable,\n",
    "        filter_value: float = -float(\"Inf\"), \n",
    "        min_tokens_to_keep: int = 1,\n",
    "        free_tokens=['!', ',', ':', '?', ';', ' ',],\n",
    "        new_line_token = '\\n',\n",
    "        num_beams: int = 10,\n",
    "    ):\n",
    "#         if not isinstance(syllable_budget, int) or syllable_budget < 0:\n",
    "#             raise ValueError(f\"`syllable_budget` has to be a strictly positive or zero integer, but is {syllable_budget}\")\n",
    "\n",
    "        self.syllable_budget = syllable_budget.repeat(num_beams)\n",
    "        self.filter_value = filter_value\n",
    "        self.syllable_scores = syllable_scorer(tokenizer, pt=True, free_tokens=free_tokens)#.repeat(num_beams, 1)\n",
    "        \n",
    "        self.new_line_token = '\\n'\n",
    "        self.new_line_token_id = tokenizer('\\n', return_tensors='pt')['input_ids'].item()\n",
    "        \n",
    "        print('prompt in warper ', prompt)\n",
    "        \n",
    "        self.prompt_offset = tokenizer(prompt, return_tensors='pt')['input_ids'].shape[1]\n",
    "        \n",
    "        self.line_number = 0\n",
    "        self.line_budget = syllable_budget.shape[0] - 1\n",
    "        print(f\"The line budget is: {self.line_budget}\")\n",
    "        \n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        batch_size = scores.shape[0]\n",
    "\n",
    "\n",
    "        syllable_scores = self.syllable_scores\n",
    "        \n",
    "        if self.line_number > self.syllable_budget.shape[0] - 1:\n",
    "            syllable_budget = torch.Tensor([0])\n",
    "        else:\n",
    "            syllable_budget = self.syllable_budget[self.line_number, None]\n",
    "\n",
    "        # Update syllable budget \n",
    "        syllable_budget = self.update_syllable_budget(input_ids, syllable_budget, syllable_scores)\n",
    "        \n",
    "        \n",
    "        # Remove all tokens with more syllables than `syllable_budget`\n",
    "        syllable_scores = syllable_scores.repeat(batch_size, 1)\n",
    "        indices_to_remove =  syllable_scores > syllable_budget[:,None]\n",
    "        \n",
    "        # Check if line has been completed\n",
    "        line_completed = syllable_budget <= 0\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "\n",
    "        if True in line_completed:\n",
    "  \n",
    "            \n",
    "            # Force EOS if line budget is spent\n",
    "            if line_budget < 0 or self.line_number > self.syllable_budget.shape[0]:\n",
    "\n",
    "                indices_to_remove[line_completed,:] = torch.full_like(indices_to_remove[line_completed,:], True)\n",
    "                scores[line_completed, tokenizer.eos_token_id] = -1\n",
    "            # Otherwise, force new line and move to next line budget\n",
    "            else:\n",
    "                scores[line_completed,  self.new_line_token_id] = -1\n",
    "                \n",
    "            self.line_number += 1\n",
    "            self.line_budget -= 1\n",
    "\n",
    "        return scores\n",
    "\n",
    "    \n",
    "    def update_syllable_budget(self, input_ids, syllable_budget, syllable_scores):\n",
    "        if input_ids.shape[1] > self.prompt_offset:\n",
    "            syllable_cost = syllable_scores[input_ids[:,-1]]\n",
    "            syllable_budget -= syllable_cost\n",
    "\n",
    "        return syllable_budget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 783,
   "id": "75c1f421-8477-4f5d-9e96-334eeef47a1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from typing import Optional, List\n",
    "\n",
    "# from bragi.verse_parsers import PoesyParsedVerseHandler\n",
    "\n",
    "\n",
    "# class MetricGenerator():\n",
    "#     def __init__(\n",
    "#         self, \n",
    "#         model, \n",
    "#         tokenizer, \n",
    "#         syllable_scorer\n",
    "#     ):\n",
    "#         self.model = model\n",
    "#         self.tokenizer = tokenizer\n",
    "#         self.syllable_scorer = syllable_scorer\n",
    "#         self.verse_handler = verse_handler = PoesyParsedVerseHandler()\n",
    "        \n",
    "    \n",
    "#     def _calculate_syllable_budget(\n",
    "#         self,\n",
    "#         text_init\n",
    "#     ):\n",
    "        \n",
    "#         _, syllable_budget = self.verse_handler.example(text_init)\n",
    "#         syllable_budget = torch.Tensor(syllable_budget)\n",
    "#         return syllable_budget\n",
    "    \n",
    "#     def generate(\n",
    "#         self, \n",
    "#         prompt, \n",
    "#         text_init: Optional[str] = None,\n",
    "#         syllable_budget: Optional[torch.Tensor] = None, \n",
    "#         free_tokens: Optional[List] = ['\\n', '!', ',', ':', '?', ';', ' '],\n",
    "#         num_beams: Optional[int] = 1, \n",
    "#         **kwargs\n",
    "#     ):\n",
    "        \n",
    "#         if text_init and syllable_budget:\n",
    "#             raise Error(\"You cannot specify both `syllable_budget` and `text_init`. Choose one or the other.\")\n",
    "        \n",
    "#         if not text_init and not torch.is_tensor(syllable_budget):\n",
    "#             raise Error(\"You must provide either `syllable_budget` or `text_init`.\")\n",
    "        \n",
    "#         if text_init:\n",
    "#             syllable_budget = self._calculate_syllable_budget(text_init)\n",
    "            \n",
    "#         processors = LogitsProcessorList()\n",
    "        \n",
    "#         processors.append(\n",
    "#             SyllableRestrictionWarper(\n",
    "#                 tokenizer=tokenizer,\n",
    "#                 syllable_budget=syllable_budget,\n",
    "#                 syllable_scorer=syllable_scorer,\n",
    "#                 free_tokens=free_tokens,\n",
    "#                 num_beams = num_beams,\n",
    "#                 prompt = prompt,\n",
    "#             )\n",
    "\n",
    "#         )\n",
    "        \n",
    "#         input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "#         outputs = model.generate(\n",
    "#             input_ids,\n",
    "#             num_beams=num_beams,\n",
    "#             logits_processor=processors,\n",
    "#             **kwargs\n",
    "#         )\n",
    "\n",
    "#         return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 845,
   "id": "9d389fea-49cc-4a45-a626-7d8339d06a0f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.token_syllable_scores(tokenizer, pt=True, free_tokens=['\\n', '!', ',', ':', '?', ';', ' '])>"
      ]
     },
     "execution_count": 845,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "syllable_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 877,
   "id": "a6d8a922-c852-4f25-9175-81e5b6aed3ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "device = 'cuda:1'\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "ebd217af-44d6-4233-a97b-cf39680cab5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from bragi.metric_generator import MetricGenerator\n",
    "generator = MetricGenerator(model=model, tokenizer=tokenizer, device=device)#, syllable_scorer=syllable_scorer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 882,
   "id": "209aa404-5a03-4332-bff9-2a083d1b5ed1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from bragi.verse_parsers import token_syllable_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 897,
   "id": "74eb5af8-4e8f-464f-9cef-a887a04b6413",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I really loved this book\n",
      "you should buy it: It's\n",
      "the best i love about all\n",
      "about your little girl\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_init = \"Crappy birthday to you,\\nHappy birthday to you,\\nHappy birthday dear Marvin,\\nHappy birthday to you\"\n",
    "prompt = \"\"\"Happy birthday to you\\n\"\"\"\n",
    "\n",
    "output = generator.generate(\n",
    "    prompt = prompt,\n",
    "    text_init = text_init,\n",
    "    # syllable_budget = torch.Tensor([6., 6.]),\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    do_sample=True,\n",
    "    max_length = 200,\n",
    ")\n",
    "\n",
    "print(output)\n",
    "# print(tokenizer.decode(output[0], skip_special_tokens=True).strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 898,
   "id": "75e65505-c1c1-47ca-b155-d448b4078f56",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Syllables per line in output: tensor([6., 5., 7., 6.])\n",
      "Syllables per line in `text_init`: tensor([6., 6., 7., 6.])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Syllables per line in output: {generator.calculate_syllable_budget(output)}\")\n",
    "print(f\"Syllables per line in `text_init`: {generator.calculate_syllable_budget(text_init)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 715,
   "id": "0f9321be",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt in warper  Happy birthday to you\n",
      "\n",
      "The line budget is: 1\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Happy birthday to you\n",
      "I'm so happy that\n",
      "You are my friend, I love\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer\n",
    "syllable_budget = torch.Tensor([6., 6.])\n",
    "syllable_scorer = token_syllable_scores\n",
    "free_tokens=['\\n', '!', ',', ':', '?', ';', ' ']\n",
    "prompt = \"\"\"Happy birthday to you\\n\"\"\"\n",
    "num_beams = 1\n",
    "\n",
    "processors = LogitsProcessorList()\n",
    "processors.append(\n",
    "    SyllableRestrictionWarper(\n",
    "        tokenizer=tokenizer,\n",
    "        syllable_budget=syllable_budget,\n",
    "        syllable_scorer=syllable_scorer,\n",
    "        free_tokens=free_tokens,\n",
    "        num_beams = num_beams,\n",
    "        prompt = prompt,\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    logits_processor=processors,\n",
    "    do_sample=False\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "id": "d91806fc-1296-4dea-a101-81f41e57d806",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_init:  \"Happy birthday to you,\n",
      "Happy birthday to you,\n",
      "Happy birthday dear Marvin,\n",
      "Happy birthday to you\" \n",
      "Syllable budget tensor([6., 6., 7., 6.])\n"
     ]
    }
   ],
   "source": [
    "text_init = \"Happy birthday to you,\\nHappy birthday to you,\\nHappy birthday dear Marvin,\\nHappy birthday to you\"\n",
    "\n",
    "# from poesy import Poem\n",
    "from bragi.verse_parsers import PoesyParsedVerseHandler\n",
    "verse_handler = PoesyParsedVerseHandler()\n",
    "_, syllable_budget = verse_handler.example(text_init)\n",
    "syllable_budget = torch.Tensor(syllable_budget)\n",
    "print('text_init: ', f'\"{text_init}\"', '\\nSyllable budget', syllable_budget)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384eba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKLogitsWarper(LogitsWarper):\n",
    "    r\"\"\"\n",
    "    [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.\n",
    "    Args:\n",
    "        top_k (`int`):\n",
    "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "        filter_value (`float`, *optional*, defaults to `-float(\"Inf\")`):\n",
    "            All filtered values will be set to this float value.\n",
    "        min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
    "            Minimum number of tokens that cannot be filtered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "        if not isinstance(top_k, int) or top_k <= 0:\n",
    "            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n",
    "\n",
    "        self.top_k = max(top_k, min_tokens_to_keep)\n",
    "        self.filter_value = filter_value\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        top_k = min(self.top_k, scores.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd95dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "global_rng = random.Random()\n",
    "\n",
    "def ids_tensor(shape, vocab_size, rng=None, name=None):\n",
    "    #  Creates a random int32 tensor of the shape within the vocab size\n",
    "    if rng is None:\n",
    "        rng = global_rng\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    return torch.tensor(data=values, dtype=torch.long, device=torch_device).view(shape).contiguous()\n",
    "\n",
    "\n",
    "class LogitsProcessorTest(unittest.TestCase):\n",
    "    def _get_uniform_logits(self, batch_size: int, length: int):\n",
    "        scores = torch.ones((batch_size, length), device=torch_device, dtype=torch.float) / length\n",
    "        return scores\n",
    "    \n",
    "    def test_min_length_dist_processor(self):\n",
    "            vocab_size = 20\n",
    "            batch_size = 4\n",
    "            eos_token_id = 0\n",
    "\n",
    "            min_dist_processor = MinLengthLogitsProcessor(min_length=10, eos_token_id=eos_token_id)\n",
    "\n",
    "            # check that min length is applied at length 5\n",
    "            input_ids = ids_tensor((batch_size, 5), vocab_size=20)\n",
    "            scores = self._get_uniform_logits(batch_size, vocab_size)\n",
    "            scores_before_min_length = min_dist_processor(input_ids, scores)\n",
    "            assert scores_before_min_length[:, eos_token_id].tolist() == 4 * [-float(\"inf\")]\n",
    "\n",
    "            # check that min length is not applied anymore at length 15\n",
    "            input_ids = ids_tensor((batch_size, 15), vocab_size=20)\n",
    "            scores = self._get_uniform_logits(batch_size, vocab_size)\n",
    "            scores_before_min_length = min_dist_processor(input_ids, scores)\n",
    "            assert not torch.isinf(scores_before_min_length).any()\n",
    "            \n",
    "    def test_top_k_dist_warper(self):\n",
    "            input_ids = None\n",
    "            vocab_size = 10\n",
    "            batch_size = 2\n",
    "\n",
    "            # create ramp distribution\n",
    "            ramp_logits = (\n",
    "                torch.arange(vocab_size, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1)\n",
    "            )\n",
    "            ramp_logits[1:, : vocab_size // 2] = ramp_logits[1:, : vocab_size // 2] + vocab_size\n",
    "\n",
    "            top_k_warp = TopKLogitsWarper(3)\n",
    "\n",
    "            scores = top_k_warp(input_ids, ramp_logits)\n",
    "            \n",
    "            return scores\n",
    "\n",
    "            # check that correct tokens are filtered\n",
    "            self.assertListEqual(torch.isinf(scores[0]).tolist(), 7 * [True] + 3 * [False])\n",
    "            self.assertListEqual(torch.isinf(scores[1]).tolist(), 2 * [True] + 3 * [False] + 5 * [True])\n",
    "\n",
    "            # check special cases\n",
    "            length = 5\n",
    "\n",
    "            logits = self._get_uniform_logits(batch_size=batch_size, length=length)\n",
    "            top_k_warp_safety_check = TopKLogitsWarper(top_k=1, filter_value=0.0, min_tokens_to_keep=3)\n",
    "\n",
    "            scores = top_k_warp_safety_check(input_ids, logits)\n",
    "            # uniform dist is not changed\n",
    "            self.assertListEqual((scores == 0.0).to(torch.long).sum(dim=-1).tolist(), [0, 0])\n",
    "\n",
    "            ramp_logits = torch.arange(length, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1)\n",
    "            scores = top_k_warp_safety_check(input_ids, ramp_logits)\n",
    "\n",
    "            # min_tokens overwrites k: 3 tokens are kept => 2 tokens are nullified\n",
    "            self.assertListEqual((scores == 0.0).to(torch.long).sum(dim=-1).tolist(), [2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a8d885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test = LogitsProcessorTest()\n",
    "lp_test.test_min_length_dist_processor()\n",
    "lp_test.test_top_k_dist_warper()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
