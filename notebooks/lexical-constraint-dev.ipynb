{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating verse with a constrained vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many reasons, we might want to constrain the vocabulary of our model at inference time. For example, we might want to only sample tokens that are common English words. Or, perhaps we want to introduce hard thematic constraints by including only words from a certain domain. `MetricGenerator` allows us to do this easily with the `tokens_to_include` argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To demonstrate, we'll use GPT2 to generate a song with and without constrained vocabularies. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's setup our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from bragi.metric_generator import MetricGenerator\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "device = 'cpu'\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize a `MetricGenerator` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator = MetricGenerator(model=model, tokenizer=tokenizer, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's generate lyrics without a lexical constraint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----output-----\n",
      "I heard there were four of\n",
      "a four hundred dogs on\n",
      "it was hard for me to get\n",
      "to the end of the day\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "text_init = \"Happy birthday to you,\\nHappy birthday to you,\\nHappy birthday dear Marvin,\\nHappy birthday to you\"\n",
    "prompt = \"\"\"This is a song about dogs:\\n\"\"\"\n",
    "\n",
    "torch.manual_seed(2)\n",
    "output = generator(\n",
    "    prompt = prompt,\n",
    "    text_init = text_init,\n",
    "    free_tokens=['||', '?', '.', ','],\n",
    "    # syllable_budget = torch.Tensor([6., 6.]),\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    remove_invalid_values=True,\n",
    "    do_sample=True,\n",
    "    top_k=25,\n",
    "    temperature=.7,\n",
    "    max_length = 100,\n",
    "    new_line_token='||',\n",
    "    bad_words_ids=[[8876]],\n",
    ")\n",
    "\n",
    "print('----output-----')\n",
    "print(output)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try constraining our vocabulary to something silly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---prompt---\n",
      "This is a song about dogs:\n",
      "\n",
      "\n",
      "---text_init----\n",
      "Happy birthday to you,\n",
      "Happy birthday to you,\n",
      "Happy birthday dear Marvin,\n",
      "Happy birthday to you\n",
      "\n",
      "\n",
      "----output-----\n",
      "To dogs, to dogs. Dogs, dogs\n",
      ". To dogs? To Dogs. From dogs\n",
      "to Dogs? From Dogs to? To? From\n",
      "To?? to? from to to To\n",
      "\n",
      "\n",
      "----Syllables-----\n",
      "Syllables per line in output: tensor([6., 6., 7., 6.])\n",
      "Syllables per line in `text_init`: tensor([6., 6., 7., 6.])\n"
     ]
    }
   ],
   "source": [
    "tokens_to_include=[\"dogs\", \"run\", \"pant\", \"lick\", \"jump\", \"to\", \"from\", \"fast\", \"now\", \"good\"]\n",
    "\n",
    "torch.manual_seed(2)\n",
    "output = generator(\n",
    "    prompt = prompt,\n",
    "    text_init = text_init,\n",
    "    free_tokens=['||', '?', '.', ','],\n",
    "    # syllable_budget = torch.Tensor([6., 6.]),\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    remove_invalid_values=True,\n",
    "    do_sample=True,\n",
    "    top_k=25,\n",
    "    temperature=.7,\n",
    "    max_length = 100,\n",
    "    new_line_token='||',\n",
    "    bad_words_ids=[[8876]],\n",
    "    tokens_to_include=tokens_to_include\n",
    ")\n",
    "\n",
    "print('---prompt---')\n",
    "print(prompt.strip())\n",
    "print('\\n')\n",
    "\n",
    "print('---text_init----')\n",
    "print(text_init)\n",
    "print('\\n')\n",
    "\n",
    "print('----output-----')\n",
    "print(output)\n",
    "print('\\n')\n",
    "\n",
    "print('----Syllables-----')\n",
    "print(f\"Syllables per line in output: {generator.calculate_syllable_budget(output)}\")\n",
    "print(f\"Syllables per line in `text_init`: {generator.calculate_syllable_budget(text_init)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neat! Now let's try something a little more expansive. We'll restrict the model vocabulary to the 10k most common English words. To do that, I'll pull a common words list to use as a lexicon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import urllib  \n",
    "\n",
    "# Pull 100,000 most frequently used English words from this resource\n",
    "# Note, it's old and not necessarily authoritative. This is just an example\n",
    "lexicon_url = \"https://gist.githubusercontent.com/h3xx/1976236/raw/bbabb412261386673eff521dddbe1dc815373b1d/wiki-100k.txt\"\n",
    "\n",
    "# Read data\n",
    "data = urllib.request.urlopen(lexicon_url) # it's a file like object and works just like a file\n",
    "tokens_to_include = []\n",
    "for line in data: # files are iterable\n",
    "    line = line.decode('utf8')\n",
    "    if '#' not in line:\n",
    "        # We'll remove new lines and lower case each word.\n",
    "        tokens_to_include.append(line.replace('\\n', '').lower())\n",
    "        \n",
    "# Since we lowered words, we probably have duplicates.\n",
    "tokens_to_include = list(set(words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "top10k_words=tokens_to_include[0:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---prompt---\n",
      "This is a song about dogs:\n",
      "\n",
      "\n",
      "---text_init----\n",
      "Happy birthday to you,\n",
      "Happy birthday to you,\n",
      "Happy birthday dear Marvin,\n",
      "Happy birthday to you\n",
      "\n",
      "\n",
      "----output-----\n",
      "The song, Love, was about\n",
      "The dogs love. They love the\n",
      "love they were.They love their love\n",
      "Love. The dogs loves their dogs\n",
      "\n",
      "\n",
      "----Syllables-----\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Syllables per line in output: tensor([6., 6., 8., 6.])\n",
      "Syllables per line in `text_init`: tensor([6., 6., 7., 6.])\n"
     ]
    }
   ],
   "source": [
    "tokens_to_include=tokens_to_include[0:1000]\n",
    "\n",
    "torch.manual_seed(2)\n",
    "output = generator(\n",
    "    prompt = prompt,\n",
    "    text_init = text_init,\n",
    "    free_tokens=['||', '?', '.', ','],\n",
    "    # syllable_budget = torch.Tensor([6., 6.]),\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=2,\n",
    "    remove_invalid_values=True,\n",
    "    do_sample=True,\n",
    "    # top_k=25,\n",
    "    temperature=.7,\n",
    "    max_length = 100,\n",
    "    new_line_token='||',\n",
    "    bad_words_ids=[[8876]],\n",
    "    tokens_to_include=top10k_words\n",
    ")\n",
    "\n",
    "print('---prompt---')\n",
    "print(prompt.strip())\n",
    "print('\\n')\n",
    "\n",
    "print('---text_init----')\n",
    "print(text_init)\n",
    "print('\\n')\n",
    "\n",
    "print('----output-----')\n",
    "print(output)\n",
    "print('\\n')\n",
    "\n",
    "print('----Syllables-----')\n",
    "print(f\"Syllables per line in output: {generator.calculate_syllable_budget(output)}\")\n",
    "print(f\"Syllables per line in `text_init`: {generator.calculate_syllable_budget(text_init)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bragi-py310",
   "language": "python",
   "name": "bragi-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
