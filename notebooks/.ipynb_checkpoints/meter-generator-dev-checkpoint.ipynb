{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54733ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b543008b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pronouncing\n",
    "\n",
    "from transformers.generation.logits_process import LogitsProcessorList\n",
    "import torch\n",
    "\n",
    "from transformers.testing_utils import require_torch, torch_device\n",
    "import random\n",
    "\n",
    "from transformers.generation.logits_process import MinLengthLogitsProcessor, TopKLogitsWarper\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa721c",
   "metadata": {},
   "source": [
    "To minimize computational cost and latency, we'll develop with (`tiny-gpt2`)[https://huggingface.co/sshleifer/tiny-gpt2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591ed86e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4f3266d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%capture\n",
    "# pip install unittest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3191a7d8",
   "metadata": {},
   "source": [
    "Define some functions for getting syllables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "f23d62b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmu_syllable_counter(word):\n",
    "    \"\"\"\n",
    "    Returns inf for OOV tokens.\n",
    "    Note: This prohibits things like numbers and punctuation. Very naive and dumb.\n",
    "    \"\"\"\n",
    "    pronunciation_list = pronouncing.phones_for_word(word)\n",
    "    if len(pronunciation_list) > 0:\n",
    "        syllable_count = pronouncing.syllable_count(pronunciation_list[0])\n",
    "    else:\n",
    "        return float(\"Inf\")\n",
    "    \n",
    "    return syllable_count\n",
    "\n",
    "def syllable_mapper(vocab):\n",
    "    syllable_map = {}\n",
    "    for token, idx in vocab.items():\n",
    "        n_syllables = cmu_syllable_counter(token)\n",
    "        try:\n",
    "            syllable_map[n_syllables].append(idx)\n",
    "        except KeyError:\n",
    "            syllable_map[n_syllables] = [idx]\n",
    "    return syllable_map\n",
    "\n",
    "def token_syllable_scores(tokenizer, pt=True, free_tokens=['\\n', '!', ',', ':', '?', ';', ' ']):\n",
    "    \"\"\"\n",
    "    Returns list or torch tensor of size==tokenizer.vocab_size where element i is the count of syllables\n",
    "    for token i.\n",
    "    \"\"\"\n",
    "    sorted_vocab = {k: v for k, v in sorted(tokenizer.vocab.items(), key=lambda item: item[1])}\n",
    "    syllable_scores = []\n",
    "    for token, idx in sorted_vocab.items():\n",
    "\n",
    "        # Have to decode the vocab item to deal with special characters, e.g. '\\n' is represented as 'ÄŠ'\n",
    "        \n",
    "   \n",
    "        decoded_token = tokenizer.decode(sorted_vocab[token])\n",
    "        if decoded_token != ' ':\n",
    "            decoded_token = decoded_token.strip()\n",
    "        if decoded_token not in free_tokens:\n",
    "            n_syllables = cmu_syllable_counter(decoded_token)\n",
    "        else:\n",
    "            n_syllables = 0\n",
    "        syllable_scores.append(n_syllables)\n",
    "        \n",
    "    if pt:\n",
    "        return torch.Tensor(syllable_scores)\n",
    "    return syllable_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "03b29e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_syllable_mapper():\n",
    "    vocab = {'living': 0, 'on': 1, 'the': 2, \"road\": 3}\n",
    "    \n",
    "    def convert_ids_to_tokens(token_id, vocab):\n",
    "        return [token for token in vocab if vocab[token]==token_id][0]\n",
    "    \n",
    "    syllable_map = syllable_mapper(vocab)\n",
    "    assert len(syllable_map[1]) == 3\n",
    "    assert convert_ids_to_tokens(syllable_map[2][0], vocab) == 'living'\n",
    "    \n",
    "\n",
    "\n",
    "class TestTokenizer():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.vocab = {'living': 0, 'on': 1, 'the': 2, \"road\": 5, \"!\": 4, \"corpus\": 3}\n",
    "    \n",
    "    def decode(self, token_id): \n",
    "        return [k for k,v in self.vocab.items() if v==token_id][0]\n",
    "    \n",
    "def test_token_syllable_scores():\n",
    "    tokenizer = TestTokenizer()\n",
    "    vocab = {'living': 0, 'on': 1, 'the': 2, \"road\": 5, \"!\": 4, \"corpus\": 3}\n",
    "    \n",
    "    syllable_scores = token_syllable_scores(tokenizer, pt=False)\n",
    "    assert syllable_scores[0] == 2\n",
    "    assert syllable_scores[1] == 1\n",
    "    assert syllable_scores[3] == 2\n",
    "    assert syllable_scores[4] == 0\n",
    "    \n",
    "test_syllable_mapper()\n",
    "test_token_syllable_scores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "33f06609",
   "metadata": {},
   "outputs": [],
   "source": [
    "syllable_scores = token_syllable_scores(tokenizer, pt=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb00b0d",
   "metadata": {},
   "source": [
    "Let's demonstrate how this works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "8f16dfc4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A word with the max number of syllables is 'homosexuality', which has inf syllables. It has a score of 7.0.\n"
     ]
    }
   ],
   "source": [
    "# Bug now that max is inf\n",
    "inf_mask = syllable_scores == float(\"Inf\")\n",
    "temp_scores = syllable_scores.masked_fill(inf_mask, -float('Inf'))\n",
    "max_word = temp_scores.argmax()\n",
    "word = tokenizer.decode(max_word.item())\n",
    "print(f\"A word with the max number of syllables is '{word.strip()}', which has {cmu_syllable_counter(word)} syllables. It has a score of {temp_scores[max_word.item()]}.\")\n",
    "                     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "2604da8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "0d08693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Happy birthday to you\n",
    "Happy birthday to you\n",
    "Happy birthday dear John\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "f4ce3195",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_init = \"\"\"Happy birthday to you\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8570b05b",
   "metadata": {},
   "source": [
    "Here's some code I wrote based on the `poesy` package that returns syllable counts by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "fcaf1927",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text_init:  \"Happy birthday to you\" \n",
      "Syllable budget tensor([6.])\n"
     ]
    }
   ],
   "source": [
    "# from poesy import Poem\n",
    "from bragi.verse_parsers import PoesyParsedVerseHandler\n",
    "verse_handler = PoesyParsedVerseHandler()\n",
    "_, syllable_budget = verse_handler.example(text_init)\n",
    "syllable_budget = torch.Tensor(syllable_budget)\n",
    "print('text_init: ', f'\"{text_init}\"', '\\nSyllable budget', syllable_budget)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecac4bd1",
   "metadata": {},
   "source": [
    "Here, I define a LogitsWarper that tracks syllable counts by line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "2e16e7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers.generation.logits_process import LogitsWarper\n",
    "\n",
    "# TODO\n",
    "# 1. Add \\n to allowed tokens\n",
    "# 2. Force \\n when line-level budget is spent\n",
    "# 3. Track total budget or n lines\n",
    "class SyllableRestrictionWarper(LogitsWarper):\n",
    "    def __init__(\n",
    "        self, \n",
    "        prompt: str,\n",
    "        tokenizer: transformers.PreTrainedTokenizerFast,\n",
    "        syllable_budget: torch.Tensor,\n",
    "        syllable_scorer: callable,\n",
    "        filter_value: float = -float(\"Inf\"), \n",
    "        min_tokens_to_keep: int = 1,\n",
    "        free_tokens=['!', ',', ':', '?', ';', ' ',],\n",
    "        new_line_token = '\\n',\n",
    "        num_beams: int = 10,\n",
    "    ):\n",
    "#         if not isinstance(syllable_budget, int) or syllable_budget < 0:\n",
    "#             raise ValueError(f\"`syllable_budget` has to be a strictly positive or zero integer, but is {syllable_budget}\")\n",
    "\n",
    "        self.syllable_budget = syllable_budget.repeat(num_beams)\n",
    "        self.filter_value = filter_value\n",
    "        self.syllable_scores = syllable_scorer(tokenizer, pt=True, free_tokens=free_tokens)#.repeat(num_beams, 1)\n",
    "        \n",
    "        self.new_line_token = '\\n'\n",
    "        \n",
    "        self.prompt_offset = tokenizer(prompt, return_tensors='pt')['input_ids'].shape[1]\n",
    "        \n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        batch_size = scores.shape[0]\n",
    "        syllable_scores = self.syllable_scores\n",
    "        syllable_budget = self.syllable_budget\n",
    "        syllable_budget = self.update_syllable_budget(input_ids, syllable_budget, syllable_scores)\n",
    "        \n",
    "\n",
    "        \n",
    "        # Remove all tokens with more syllables than `syllable_budget`\n",
    "        syllable_scores = syllable_scores.repeat(batch_size, 1)\n",
    "        indices_to_remove =  syllable_scores > syllable_budget[:,None]\n",
    "        \n",
    "        # Check if line has been completed\n",
    "        line_completed = syllable_budget <= 0\n",
    "        # For completed lines, force EOS\n",
    "        indices_to_remove[line_completed,:] = torch.full_like(indices_to_remove[line_completed,:], True)\n",
    "        indices_to_remove[line_completed, tokenizer.eos_token_id] = False\n",
    "\n",
    "        # Mask scores\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "        \n",
    "        # Update budget\n",
    "        self.syllable_budget = syllable_budget\n",
    "        return scores\n",
    "\n",
    "    \n",
    "    def update_syllable_budget(self, input_ids, syllable_budget, syllable_scores):\n",
    "        print('init ', syllable_budget)\n",
    "        if input_ids.shape[1] > self.prompt_offset:\n",
    "            syllable_cost = syllable_scores[input_ids[:,-1]]\n",
    "            syllable_budget -= syllable_cost\n",
    "        print([tokenizer.decode(t) for t in input_ids[:,-1]])\n",
    "        print('---'*10)\n",
    "        print('new syllable_budget ', syllable_budget)\n",
    "        return syllable_budget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "0f9321be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init  tensor([6.])\n",
      "['\\n']\n",
      "------------------------------\n",
      "new syllable_budget  tensor([6.])\n",
      "init  tensor([6.])\n",
      "['I']\n",
      "------------------------------\n",
      "new syllable_budget  tensor([5.])\n",
      "init  tensor([5.])\n",
      "[' know']\n",
      "------------------------------\n",
      "new syllable_budget  tensor([4.])\n",
      "init  tensor([4.])\n",
      "[' this']\n",
      "------------------------------\n",
      "new syllable_budget  tensor([3.])\n",
      "init  tensor([3.])\n",
      "[' is']\n",
      "------------------------------\n",
      "new syllable_budget  tensor([2.])\n",
      "init  tensor([2.])\n",
      "[' an']\n",
      "------------------------------\n",
      "new syllable_budget  tensor([1.])\n",
      "init  tensor([1.])\n",
      "[' old']\n",
      "------------------------------\n",
      "new syllable_budget  tensor([0.])\n",
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Happy birthday to you\n",
      "I know this is an old\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tokenizer\n",
    "syllable_budget = 1\n",
    "syllable_scorer = token_syllable_scores\n",
    "free_tokens=['\\n', '!', ',', ':', '?', ';', ' ']\n",
    "num_beams = 1\n",
    "prompt = \"\"\"Happy birthday to you\\n\"\"\"\n",
    "\n",
    "processors = LogitsProcessorList()\n",
    "processors.append(\n",
    "    SyllableRestrictionWarper(\n",
    "        tokenizer=tokenizer,\n",
    "        syllable_budget=torch.Tensor([6.]),\n",
    "        syllable_scorer=syllable_scorer,\n",
    "        free_tokens=free_tokens,\n",
    "        num_beams = num_beams,\n",
    "        prompt = prompt,\n",
    "    )\n",
    "\n",
    ")\n",
    "\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "outputs = model.generate(\n",
    "    input_ids,\n",
    "    num_beams=num_beams,\n",
    "    num_return_sequences=1,\n",
    "    no_repeat_ngram_size=1,\n",
    "    remove_invalid_values=True,\n",
    "    logits_processor=processors,\n",
    "    do_sample=True\n",
    ")\n",
    "\n",
    "print(\"Output:\\n\" + 100 * '-')\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384eba0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TopKLogitsWarper(LogitsWarper):\n",
    "    r\"\"\"\n",
    "    [`LogitsWarper`] that performs top-k, i.e. restricting to the k highest probability elements.\n",
    "    Args:\n",
    "        top_k (`int`):\n",
    "            The number of highest probability vocabulary tokens to keep for top-k-filtering.\n",
    "        filter_value (`float`, *optional*, defaults to `-float(\"Inf\")`):\n",
    "            All filtered values will be set to this float value.\n",
    "        min_tokens_to_keep (`int`, *optional*, defaults to 1):\n",
    "            Minimum number of tokens that cannot be filtered.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, top_k: int, filter_value: float = -float(\"Inf\"), min_tokens_to_keep: int = 1):\n",
    "        if not isinstance(top_k, int) or top_k <= 0:\n",
    "            raise ValueError(f\"`top_k` has to be a strictly positive integer, but is {top_k}\")\n",
    "\n",
    "        self.top_k = max(top_k, min_tokens_to_keep)\n",
    "        self.filter_value = filter_value\n",
    "\n",
    "    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        top_k = min(self.top_k, scores.size(-1))  # Safety check\n",
    "        # Remove all tokens with a probability less than the last token of the top-k\n",
    "        indices_to_remove = scores < torch.topk(scores, top_k)[0][..., -1, None]\n",
    "        scores = scores.masked_fill(indices_to_remove, self.filter_value)\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "bd95dd4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "global_rng = random.Random()\n",
    "\n",
    "def ids_tensor(shape, vocab_size, rng=None, name=None):\n",
    "    #  Creates a random int32 tensor of the shape within the vocab size\n",
    "    if rng is None:\n",
    "        rng = global_rng\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    return torch.tensor(data=values, dtype=torch.long, device=torch_device).view(shape).contiguous()\n",
    "\n",
    "\n",
    "class LogitsProcessorTest(unittest.TestCase):\n",
    "    def _get_uniform_logits(self, batch_size: int, length: int):\n",
    "        scores = torch.ones((batch_size, length), device=torch_device, dtype=torch.float) / length\n",
    "        return scores\n",
    "    \n",
    "    def test_min_length_dist_processor(self):\n",
    "            vocab_size = 20\n",
    "            batch_size = 4\n",
    "            eos_token_id = 0\n",
    "\n",
    "            min_dist_processor = MinLengthLogitsProcessor(min_length=10, eos_token_id=eos_token_id)\n",
    "\n",
    "            # check that min length is applied at length 5\n",
    "            input_ids = ids_tensor((batch_size, 5), vocab_size=20)\n",
    "            scores = self._get_uniform_logits(batch_size, vocab_size)\n",
    "            scores_before_min_length = min_dist_processor(input_ids, scores)\n",
    "            assert scores_before_min_length[:, eos_token_id].tolist() == 4 * [-float(\"inf\")]\n",
    "\n",
    "            # check that min length is not applied anymore at length 15\n",
    "            input_ids = ids_tensor((batch_size, 15), vocab_size=20)\n",
    "            scores = self._get_uniform_logits(batch_size, vocab_size)\n",
    "            scores_before_min_length = min_dist_processor(input_ids, scores)\n",
    "            assert not torch.isinf(scores_before_min_length).any()\n",
    "            \n",
    "    def test_top_k_dist_warper(self):\n",
    "            input_ids = None\n",
    "            vocab_size = 10\n",
    "            batch_size = 2\n",
    "\n",
    "            # create ramp distribution\n",
    "            ramp_logits = (\n",
    "                torch.arange(vocab_size, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1)\n",
    "            )\n",
    "            ramp_logits[1:, : vocab_size // 2] = ramp_logits[1:, : vocab_size // 2] + vocab_size\n",
    "\n",
    "            top_k_warp = TopKLogitsWarper(3)\n",
    "\n",
    "            scores = top_k_warp(input_ids, ramp_logits)\n",
    "            \n",
    "            return scores\n",
    "\n",
    "            # check that correct tokens are filtered\n",
    "            self.assertListEqual(torch.isinf(scores[0]).tolist(), 7 * [True] + 3 * [False])\n",
    "            self.assertListEqual(torch.isinf(scores[1]).tolist(), 2 * [True] + 3 * [False] + 5 * [True])\n",
    "\n",
    "            # check special cases\n",
    "            length = 5\n",
    "\n",
    "            logits = self._get_uniform_logits(batch_size=batch_size, length=length)\n",
    "            top_k_warp_safety_check = TopKLogitsWarper(top_k=1, filter_value=0.0, min_tokens_to_keep=3)\n",
    "\n",
    "            scores = top_k_warp_safety_check(input_ids, logits)\n",
    "            # uniform dist is not changed\n",
    "            self.assertListEqual((scores == 0.0).to(torch.long).sum(dim=-1).tolist(), [0, 0])\n",
    "\n",
    "            ramp_logits = torch.arange(length, device=torch_device, dtype=torch.float).unsqueeze(0).repeat(batch_size, 1)\n",
    "            scores = top_k_warp_safety_check(input_ids, ramp_logits)\n",
    "\n",
    "            # min_tokens overwrites k: 3 tokens are kept => 2 tokens are nullified\n",
    "            self.assertListEqual((scores == 0.0).to(torch.long).sum(dim=-1).tolist(), [2, 2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a8d885f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lp_test = LogitsProcessorTest()\n",
    "lp_test.test_min_length_dist_processor()\n",
    "lp_test.test_top_k_dist_warper()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bragi-py310",
   "language": "python",
   "name": "bragi-py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
